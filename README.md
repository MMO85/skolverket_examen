# skolverket_examen
CSV-pipeline med DLT och DuckDB

Det hÃ¤r projektet innehÃ¥ller en pipeline fÃ¶r dataladdning byggd med DLT (Data Load Tool) och DuckDB som mÃ¥l. Pipelinens syfte Ã¤r att lÃ¤sa in rÃ¥a CSV-filer frÃ¥n mappen raw_data/ och lagra dem i en lokal DuckDB-databas fÃ¶r vidare bearbetning.

Mappstruktur
project_root/
â”‚
â”œâ”€â”€ data_extract_load/
â”‚     â””â”€â”€ load_csv_data.py        # Huvudscript fÃ¶r att ladda in CSV-filer
â”‚
â”œâ”€â”€ raw_data/                     # RÃ¥data i CSV-format
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

FÃ¶rutsÃ¤ttningar

Skapa och aktivera ett virtual environment

Installera beroenden:

pip install -r requirements.txt


requirements.txts innehÃ¥lla:

dlt
pandas
duckdb

Om load_csv_data.py

Scriptet gÃ¶r fÃ¶ljande:

Letar upp alla CSV-filer i mappen raw_data/

LÃ¤ser varje fil med en flexibel Pythontolkare fÃ¶r att undvika lÃ¤sfel

Konverterar alla kolumner till text fÃ¶r att undvika schema-konflikter mellan olika CSV-filer

LÃ¤gger till filnamnet som en extra kolumn

KÃ¶r DLT-pipen och laddar datan till DuckDB

Skapar den lokala databasen:

csv_ingestion_pipeline.duckdb

SÃ¥ kÃ¶r du pipelinen

I projektroten, kÃ¶r:

python data_extract_load/load_csv_data.py
--$ python -c "import duckdb; con=duckdb.connect('csv_ingestion_pipeline.duckdb'); print(con.execute(\"select count(distinct _dlt_load_id) from staging_data.raw_data\").fetchone())"
(1,) den 1 an Ã¤r rÃ¤tt svar. 
(.venv)

Vid lyckad kÃ¶rning visas ett meddelande i stil med:

CSV data loaded successfully via DLT.

Inspektera datan i DuckDB

Starta Python och kÃ¶r exempelvis:

import duckdb
con = duckdb.connect("csv_ingestion_pipeline.duckdb")
con.execute("SELECT * FROM staging_data.raw_data LIMIT 10").fetchall()


DÃ¥ kan du verifiera att datan har laddats korrekt.

Ã–vrigt:

Alla CSV-filer laddas in oavsett om deras kolumnstrukturer skiljer sig Ã¥t.

Konvertering till textformat Ã¤r avsiktligt och fÃ¶ljer principen fÃ¶r rÃ¥data (raw layer).

Pipeline-utdata kan senare anvÃ¤ndas fÃ¶r att bygga silver-lager, datamodeller eller analyser.

-------------------------------------------------------------

DBT: ###  jag har Ã¤ndrat nÃ¥gra path, ser sÃ¥ hÃ¤r ut nedan, men vi kan fixa senare hela snyggare. 
Projektstruktur (kort)
skolverket_examen/
â”œâ”€â”€ data_extract_load/        # DLT-pipeline (CSV â†’ DuckDB)
â”œâ”€â”€ raw_data/                 # RÃ¥ CSV-filer (lokalt)
â”œâ”€â”€ csv_ingestion_pipeline.duckdb
â”œâ”€â”€ dbt_project/              # dbt-projekt
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ staging/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ marts/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

FÃ¶rutsÃ¤ttningar : Python 3.10+, Git, Windows (Git Bash eller PowerShell funkar)

SÃ¥ kÃ¶r du projektet (frÃ¥n noll)
1ï¸. Skapa och aktivera virtual environment
python -m venv .venv
source .venv/Scripts/activate

2ï¸. Installera beroenden
python -m pip install -r requirements.txt

 Steg 1: Ladda data med DLT

Detta lÃ¤ser alla CSV-filer i raw_data/ och skapar/uppdaterar DuckDB-filen.

python data_extract_load/load_csv_data.py


Resultat:

csv_ingestion_pipeline.duckdb skapas/uppdateras

Tabellen staging_data.raw_data innehÃ¥ller:

raw_line

source_file

Steg 2: Konfigurera dbt (en gÃ¥ng per dator)  Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø±Ø§ÛŒ Øª Ù‡Ù… Ù…Ù‡Ù…Û•

Skapa filen:

~/.dbt/profiles.yml


InnehÃ¥ll:

skolverket_examen:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: ../csv_ingestion_pipeline.duckdb
      schema: staging_data

Steg 3: KÃ¶r dbt (staging â†’ silver â†’ marts)

GÃ¥ in i dbt-mappen:

cd dbt_project


KÃ¶r modeller:

python -m dbt.cli.main run --profiles-dir "$USERPROFILE/.dbt"


KÃ¶r tester:

python -m dbt.cli.main test --profiles-dir "$USERPROFILE/.dbt"


Om allt Ã¤r rÃ¤tt ska:

dbt run bygga 16 modeller

dbt test ge PASS pÃ¥ alla tester

Designprinciper (viktigt fÃ¶r grupparbete)

stg_raw_data
Endast rÃ¥data (raw_line, source_file)
 ingen parsing

slv_cleaned_data
FÃ¶rsta â€œrensadeâ€ lagret
 URL extraheras hÃ¤r
url_value Ã¤r valfri (kan vara NULL)

schema & sources

Inga hÃ¥rdkodade scheman (main)

{{ target.schema }} anvÃ¤nds Ã¶verallt

Lokala filer ignoreras

.duckdb

target/

logs/
â†’ inga binÃ¤ra merge-konflikter

 Vanliga problem

dbt.exe funkar inte pÃ¥ Windows
â†’ anvÃ¤nd alltid:

python -m dbt.cli.main ...


dbt hittar inte projektet
â†’ se till att du kÃ¶r kommandon inifrÃ¥n dbt_project/

--- Status

âœ” DLT pipeline fungerar
âœ” dbt run & test grÃ¶na
âœ” Team-safe setup
-------------------------------------------------------
stremlit app/app.py
Data ingestion (DLT)

Transformation & modeller (dbt + DuckDB)

Visualisering (Streamlit + Plotly + GeoData)

ğŸ§± SystemÃ¶versikt

Teknikstack

Python 3.11

DuckDB (lokal analytisk databas)

DLT (data ingestion)

dbt (staging + marts)

Streamlit (dashboard)

GeoPandas + Plotly (kartvisualisering)

Git / GitHub (versionshantering)

 Projektstruktur
skolverket_examen/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                  # Streamlit-dashboard
â”‚   â””â”€â”€ geo/
â”‚       â”œâ”€â”€ raw/                # Original geojson (kommuner, lÃ¤n)
â”‚       â”œâ”€â”€ processed/          # FÃ¶renklad geo-data (parquet + geojson)
â”‚       â”œâ”€â”€ preprocess_geo.py   # Rensar & standardiserar geo-data
â”‚       â””â”€â”€ load_geo.py
â”‚
â”œâ”€â”€ data_extract_load/
â”‚   â””â”€â”€ load_csv_data.py        # DLT-pipeline (CSV â†’ DuckDB)
â”‚
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ csv_ingestion_pipeline.duckdb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

ğŸ”„ DataflÃ¶de (Steg fÃ¶r steg)
1ï¸âƒ£ Data ingestion â€“ DLT

Skolverkets CSV-filer laddas till DuckDB via DLT.

source .venv/Scripts/activate
python data_extract_load/load_csv_data.py


Resultat:

DuckDB-fil skapas/uppdateras:

csv_ingestion_pipeline.duckdb


RÃ¥data hamnar i schema staging_data

2ï¸âƒ£ Transformation â€“ dbt

dbt anvÃ¤nds fÃ¶r:

Staging (rensning, typning)

Business logic

Analytiska marts

cd dbt_project
python -m dbt.cli.main debug --profiles-dir "$USERPROFILE/.dbt"
python -m dbt.cli.main run   --profiles-dir "$USERPROFILE/.dbt"
python -m dbt.cli.main test  --profiles-dir "$USERPROFILE/.dbt"


Exempel pÃ¥ marts:

mart_ranked_kommun_ak9

mart_nationella_prov_ak9

mart_parent_trend_ak9

mart_parent_fairness_ak9

3ï¸âƒ£ Geo-data â€“ Kommuner & LÃ¤n

Sveriges kommun- och lÃ¤nsgrÃ¤nser anvÃ¤nds fÃ¶r kartan.

RÃ¥data

app/geo/raw/kommuner.geojson

app/geo/raw/lan.geojson

Bearbetning

Geo-datan:

projiceras till WGS84

trasiga geometrier fixas

fÃ¶renklas (fÃ¶r prestanda)

standardiseras sÃ¥ att kolumner matchar dbt-data

python app/geo/preprocess_geo.py


Resultat:

app/geo/processed/
â”œâ”€â”€ kommuner.parquet   # kolumner: kommun, kommun_kod, lan_kod, geometry
â”œâ”€â”€ lan.parquet        # kolumner: lan, lan_kod, geometry

ğŸ—ºï¸ Dashboard â€“ Streamlit

Dashboarden visar:

ğŸ—ºï¸ Karta

Choropleth-karta Ã¶ver Sveriges kommuner

FÃ¤rg baserat pÃ¥ score_0_100 (Ã¥k 9)

Filter:

LÃ¤sÃ¥r

Ã„mne

Huvudman

ğŸ† Ranking

Topp/Nedersta kommuner

JÃ¤mfÃ¶relser inom lÃ¤n och nationellt

ğŸ§¾ Overview

Textdata och metadata frÃ¥n kÃ¤llfiler

Starta dashboarden frÃ¥n projektroten:

python -m streamlit run app/app.py

âœ… Kvalitet & Robusthet

dbt tester (not_null, m.fl.)

SÃ¤ker SQL-escape i Streamlit

Tydlig matchning mellan geo-data och dbt-marts

Debug-sektion fÃ¶r att visa om kommuner saknar matchning
#
## Streamlit 
python app/geo/process_geo.py
python -m streamlit run app/app.py


ğŸ¯ Sammanfattning

Detta projekt demonstrerar:

End-to-end data engineering

Analytisk modellering med dbt

Geografisk visualisering

Tydlig separation mellan ingestion, transformation och presentation

Allt kÃ¶rbart lokalt, reproducerbart och granskningsbart.
--------------------------





karta.
# Skolverket Examen â€“ DuckDB + dlt + dbt + Streamlit

Det hÃ¤r projektet bygger en liten, reproducerbar datapipeline fÃ¶r Skolverkets Ã¶ppna CSV:er:

1) **DLT** lÃ¤ser alla CSV-filer i `raw_data/` och laddar raderna till **DuckDB** (`staging_data.raw_data`)  
2) Vi **synkar** alltid databasen till *senaste* kÃ¶rningen (sÃ¥ att inga gamla rader ligger kvar)  
3) **dbt** transformerar `raw_data` â†’ `stg_*` (staging) â†’ `marts` (analysmodeller)  
4) **Streamlit** visar Sverigekarta per kommun + jÃ¤mfÃ¶relse Top/Bottom

---

## Projektstruktur (Ã¶versikt)

- `raw_data/` â€“ dina CSV-filer (in-data)
- `csv_ingestion_pipeline.duckdb` â€“ DuckDB-databasen (skapas/uppdateras)
- `data_extract_load/load_csv_data.py` â€“ kÃ¶r DLT + sync + dbt run/test (end-to-end)
- `dbt_project/` â€“ dbt-projekt (modeller, macros, profiles.yml, dbt_project.yml)
- `app/app_karta.py` â€“ Streamlit-dashboard (karta + jÃ¤mfÃ¶relser)
- `app/geo/processed/kommuner.parquet` â€“ kommungeometri fÃ¶r kartan
- `preprocess_geo.py` â€“ (om du behÃ¶ver bygga om geodata/parquet)

---

## FÃ¶rkrav

- Python 3.12+
- Git
- (Windows) PowerShell/Windows Terminal rekommenderas. MSYS/Git Bash funkar men kan ge â€œPermission deniedâ€ pÃ¥ vissa entrypoints.

---

## Installation

### 1) Skapa och aktivera venv

**Windows (PowerShell):**
```bash
python -m venv .venv
.\.venv\Scripts\Activate.ps1
Git Bash/MSYS:

bash
Kopiera kod
python -m venv .venv
source .venv/Scripts/activate
2) Installera dependencies
bash
Kopiera kod
pip install -r requirements.txt
Exakta Python-paket (frÃ¥n requirements.txt)
Nedan visas exakt innehÃ¥ll frÃ¥n requirements.txt i repo-root.

<!-- REQUIREMENTS_START -->
txt
Kopiera kod
(autogenereras frÃ¥n requirements.txt)
<!-- REQUIREMENTS_END -->
Uppdatera paketlistan i README automatiskt
KÃ¶r ett av fÃ¶ljande (beroende pÃ¥ terminal):

Git Bash/MSYS (frÃ¥n repo-root):

bash
Kopiera kod
python - <<'PY'
from pathlib import Path
req = Path("requirements.txt").read_text(encoding="utf-8").rstrip()
readme = Path("README.md").read_text(encoding="utf-8")

start = "<!-- REQUIREMENTS_START -->"
end = "<!-- REQUIREMENTS_END -->"

before = readme.split(start)[0] + start
after = readme.split(end)[1]
block = "\n```txt\n" + req + "\n```\n"

Path("README.md").write_text(before + "\n" + block + end + after, encoding="utf-8")
print("README.md uppdaterad med requirements.txt âœ…")
PY
PowerShell (frÃ¥n repo-root):

powershell
Kopiera kod
python -c @"
from pathlib import Path
req = Path('requirements.txt').read_text(encoding='utf-8').rstrip()
readme = Path('README.md').read_text(encoding='utf-8')
start = '<!-- REQUIREMENTS_START -->'
end   = '<!-- REQUIREMENTS_END -->'
before = readme.split(start)[0] + start
after  = readme.split(end)[1]
block = '\n```txt\n' + req + '\n```\n'
Path('README.md').write_text(before + '\n' + block + end + after, encoding='utf-8')
print('README.md uppdaterad med requirements.txt âœ…')
"@
KÃ¶rflÃ¶de (rekommenderat)
1) Ladda data + sync + dbt (allt i ett)
bash
Kopiera kod
python data_extract_load/load_csv_data.py
Vad scriptet gÃ¶r:

LÃ¤ser alla CSV i raw_data/

Laddar rader till staging_data.raw_data via dlt

Tar bort allt utom senaste _dlt_load_id (sÃ¥ databasen alltid matchar senaste ingestion)

KÃ¶r:

python -m dbt.cli.main run

python -m dbt.cli.main test

Scriptet sÃ¤tter DBT_PROFILES_DIR till dbt_project/ sÃ¥ att dbt alltid hittar rÃ¤tt profiles.yml.

2) Starta Streamlit-dashboard
KÃ¶r frÃ¥n repo-root:

bash
Kopiera kod
python -m streamlit run app/app_karta.py
Om du fÃ¥r Permission denied pÃ¥ streamlit-kommandot (vanligt i MSYS):

anvÃ¤nd alltid python -m streamlit ... som ovan.

dbt â€“ modeller och materialisering
Staging-modeller (stg_*) Ã¤r byggda fÃ¶r att vara snabba och lÃ¤tta (ofta view)

Marts (mart_*) Ã¤r tabeller fÃ¶r stabil analys/prestanda

Centrala modeller:

mart_ranked_kommun_ak9 â€“ ranking/score per kommun (Ã¥k9)

mart_budget_per_elev_kommun â€“ budget per elev per kommun och lÃ¤sÃ¥r

Streamlit â€“ vad som visas
Dashboarden har vyer fÃ¶r:

Karta (Ranking) â€“ choropleth Ã¶ver kommuner med blÃ¥ fÃ¤rgskala

Karta (Budget per elev) â€“ choropleth + Top/Bottom-jÃ¤mfÃ¶relse mellan kommuner

MÃ¥let Ã¤r att endast visa Sverigekartan (inte vÃ¤rldskartan). Kartans center/zoom Ã¤r instÃ¤lld fÃ¶r Sverige.

Snabbcheck
Har vi data?

bash
Kopiera kod
python -c "import duckdb; con=duckdb.connect('csv_ingestion_pipeline.duckdb'); print(con.execute(\"select count(*) from staging_data.raw_data\").fetchone())"
Finns mart-tabellerna?

bash
Kopiera kod
python -c "import duckdb; con=duckdb.connect('csv_ingestion_pipeline.duckdb'); print(con.execute(\"select table_name, table_type from information_schema.tables where table_schema='staging_data' and table_name like 'mart_%' order by 1\").fetchall())"
Vanliga problem & fixar
dbt hittas inte / fel modul
AnvÃ¤nd alltid:

bash
Kopiera kod
python -m dbt.cli.main --version
python -m dbt.cli.main run
python -m dbt.cli.main test
dbt kan inte lÃ¤sa profiles.yml
bash
Kopiera kod
cd dbt_project
export DBT_PROFILES_DIR="$(pwd)"
python -m dbt.cli.main debug
dbt kan inte Ã¶ppna DuckDB-filen (fel sÃ¶kvÃ¤g)
Kontrollera path: i output frÃ¥n python -m dbt.cli.main debug.

NÃ¤sta steg (fÃ¶rbÃ¤ttringar)
â€œKlick pÃ¥ kommun â†’ detaljerâ€ (trend/tooltip + smÃ¥kort)

Mer dbt-testning (uniqueness pÃ¥ kommun_kod + lasar_start, not_null pÃ¥ viktiga mÃ¥tt)

Deployment (Streamlit Community Cloud / Docker)