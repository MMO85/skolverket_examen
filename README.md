# skolverket_examen
CSV-pipeline med DLT och DuckDB

Det hÃ¤r projektet innehÃ¥ller en pipeline fÃ¶r dataladdning byggd med DLT (Data Load Tool) och DuckDB som mÃ¥l. Pipelinens syfte Ã¤r att lÃ¤sa in rÃ¥a CSV-filer frÃ¥n mappen raw_data/ och lagra dem i en lokal DuckDB-databas fÃ¶r vidare bearbetning.

Mappstruktur
project_root/
â”‚
â”œâ”€â”€ data_extract_load/
â”‚     â””â”€â”€ load_csv_data.py        # Huvudscript fÃ¶r att ladda in CSV-filer
â”‚
â”œâ”€â”€ raw_data/                     # RÃ¥data i CSV-format
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

FÃ¶rutsÃ¤ttningar

Skapa och aktivera ett virtual environment

Installera beroenden:

pip install -r requirements.txt


requirements.txts innehÃ¥lla:

dlt
pandas
duckdb

Om load_csv_data.py

Scriptet gÃ¶r fÃ¶ljande:

Letar upp alla CSV-filer i mappen raw_data/

LÃ¤ser varje fil med en flexibel Pythontolkare fÃ¶r att undvika lÃ¤sfel

Konverterar alla kolumner till text fÃ¶r att undvika schema-konflikter mellan olika CSV-filer

LÃ¤gger till filnamnet som en extra kolumn

KÃ¶r DLT-pipen och laddar datan till DuckDB

Skapar den lokala databasen:

csv_ingestion_pipeline.duckdb

SÃ¥ kÃ¶r du pipelinen

I projektroten, kÃ¶r:

python data_extract_load/load_csv_data.py


Vid lyckad kÃ¶rning visas ett meddelande i stil med:

CSV data loaded successfully via DLT.

Inspektera datan i DuckDB

Starta Python och kÃ¶r exempelvis:

import duckdb
con = duckdb.connect("csv_ingestion_pipeline.duckdb")
con.execute("SELECT * FROM staging_data.raw_data LIMIT 10").fetchall()


DÃ¥ kan du verifiera att datan har laddats korrekt.

Ã–vrigt:

Alla CSV-filer laddas in oavsett om deras kolumnstrukturer skiljer sig Ã¥t.

Konvertering till textformat Ã¤r avsiktligt och fÃ¶ljer principen fÃ¶r rÃ¥data (raw layer).

Pipeline-utdata kan senare anvÃ¤ndas fÃ¶r att bygga silver-lager, datamodeller eller analyser.

-------------------------------------------------------------

DBT: ###  jag har Ã¤ndrat nÃ¥gra path, ser sÃ¥ hÃ¤r ut nedan, men vi kan fixa senare hela snyggare. 
Projektstruktur (kort)
skolverket_examen/
â”œâ”€â”€ data_extract_load/        # DLT-pipeline (CSV â†’ DuckDB)
â”œâ”€â”€ raw_data/                 # RÃ¥ CSV-filer (lokalt)
â”œâ”€â”€ csv_ingestion_pipeline.duckdb
â”œâ”€â”€ dbt_project/              # dbt-projekt
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ staging/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ marts/
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

FÃ¶rutsÃ¤ttningar : Python 3.10+, Git, Windows (Git Bash eller PowerShell funkar)

SÃ¥ kÃ¶r du projektet (frÃ¥n noll)
1ï¸. Skapa och aktivera virtual environment
python -m venv .venv
source .venv/Scripts/activate

2ï¸. Installera beroenden
python -m pip install -r requirements.txt

 Steg 1: Ladda data med DLT

Detta lÃ¤ser alla CSV-filer i raw_data/ och skapar/uppdaterar DuckDB-filen.

python data_extract_load/load_csv_data.py


Resultat:

csv_ingestion_pipeline.duckdb skapas/uppdateras

Tabellen staging_data.raw_data innehÃ¥ller:

raw_line

source_file

Steg 2: Konfigurera dbt (en gÃ¥ng per dator)  Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø±Ø§ÛŒ Øª Ù‡Ù… Ù…Ù‡Ù…Û•

Skapa filen:

~/.dbt/profiles.yml


InnehÃ¥ll:

skolverket_examen:
  target: dev
  outputs:
    dev:
      type: duckdb
      path: ../csv_ingestion_pipeline.duckdb
      schema: staging_data

Steg 3: KÃ¶r dbt (staging â†’ silver â†’ marts)

GÃ¥ in i dbt-mappen:

cd dbt_project


KÃ¶r modeller:

python -m dbt.cli.main run --profiles-dir "$USERPROFILE/.dbt"


KÃ¶r tester:

python -m dbt.cli.main test --profiles-dir "$USERPROFILE/.dbt"


Om allt Ã¤r rÃ¤tt ska:

dbt run bygga 16 modeller

dbt test ge PASS pÃ¥ alla tester

Designprinciper (viktigt fÃ¶r grupparbete)

stg_raw_data
Endast rÃ¥data (raw_line, source_file)
 ingen parsing

slv_cleaned_data
FÃ¶rsta â€œrensadeâ€ lagret
 URL extraheras hÃ¤r
url_value Ã¤r valfri (kan vara NULL)

schema & sources

Inga hÃ¥rdkodade scheman (main)

{{ target.schema }} anvÃ¤nds Ã¶verallt

Lokala filer ignoreras

.duckdb

target/

logs/
â†’ inga binÃ¤ra merge-konflikter

 Vanliga problem

dbt.exe funkar inte pÃ¥ Windows
â†’ anvÃ¤nd alltid:

python -m dbt.cli.main ...


dbt hittar inte projektet
â†’ se till att du kÃ¶r kommandon inifrÃ¥n dbt_project/

--- Status

âœ” DLT pipeline fungerar
âœ” dbt run & test grÃ¶na
âœ” Team-safe setup
-------------------------------------------------------
stremlit app/app.py
Data ingestion (DLT)

Transformation & modeller (dbt + DuckDB)

Visualisering (Streamlit + Plotly + GeoData)

ğŸ§± SystemÃ¶versikt

Teknikstack

Python 3.11

DuckDB (lokal analytisk databas)

DLT (data ingestion)

dbt (staging + marts)

Streamlit (dashboard)

GeoPandas + Plotly (kartvisualisering)

Git / GitHub (versionshantering)

 Projektstruktur
skolverket_examen/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ app.py                  # Streamlit-dashboard
â”‚   â””â”€â”€ geo/
â”‚       â”œâ”€â”€ raw/                # Original geojson (kommuner, lÃ¤n)
â”‚       â”œâ”€â”€ processed/          # FÃ¶renklad geo-data (parquet + geojson)
â”‚       â”œâ”€â”€ preprocess_geo.py   # Rensar & standardiserar geo-data
â”‚       â””â”€â”€ load_geo.py
â”‚
â”œâ”€â”€ data_extract_load/
â”‚   â””â”€â”€ load_csv_data.py        # DLT-pipeline (CSV â†’ DuckDB)
â”‚
â”œâ”€â”€ dbt_project/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â””â”€â”€ marts/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ csv_ingestion_pipeline.duckdb
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

ğŸ”„ DataflÃ¶de (Steg fÃ¶r steg)
1ï¸âƒ£ Data ingestion â€“ DLT

Skolverkets CSV-filer laddas till DuckDB via DLT.

source .venv/Scripts/activate
python data_extract_load/load_csv_data.py


Resultat:

DuckDB-fil skapas/uppdateras:

csv_ingestion_pipeline.duckdb


RÃ¥data hamnar i schema staging_data

2ï¸âƒ£ Transformation â€“ dbt

dbt anvÃ¤nds fÃ¶r:

Staging (rensning, typning)

Business logic

Analytiska marts

cd dbt_project
python -m dbt.cli.main debug --profiles-dir "$USERPROFILE/.dbt"
python -m dbt.cli.main run   --profiles-dir "$USERPROFILE/.dbt"
python -m dbt.cli.main test  --profiles-dir "$USERPROFILE/.dbt"


Exempel pÃ¥ marts:

mart_ranked_kommun_ak9

mart_nationella_prov_ak9

mart_parent_trend_ak9

mart_parent_fairness_ak9

3ï¸âƒ£ Geo-data â€“ Kommuner & LÃ¤n

Sveriges kommun- och lÃ¤nsgrÃ¤nser anvÃ¤nds fÃ¶r kartan.

RÃ¥data

app/geo/raw/kommuner.geojson

app/geo/raw/lan.geojson

Bearbetning

Geo-datan:

projiceras till WGS84

trasiga geometrier fixas

fÃ¶renklas (fÃ¶r prestanda)

standardiseras sÃ¥ att kolumner matchar dbt-data

python app/geo/preprocess_geo.py


Resultat:

app/geo/processed/
â”œâ”€â”€ kommuner.parquet   # kolumner: kommun, kommun_kod, lan_kod, geometry
â”œâ”€â”€ lan.parquet        # kolumner: lan, lan_kod, geometry

ğŸ—ºï¸ Dashboard â€“ Streamlit

Dashboarden visar:

ğŸ—ºï¸ Karta

Choropleth-karta Ã¶ver Sveriges kommuner

FÃ¤rg baserat pÃ¥ score_0_100 (Ã¥k 9)

Filter:

LÃ¤sÃ¥r

Ã„mne

Huvudman

ğŸ† Ranking

Topp/Nedersta kommuner

JÃ¤mfÃ¶relser inom lÃ¤n och nationellt

ğŸ§¾ Overview

Textdata och metadata frÃ¥n kÃ¤llfiler

Starta dashboarden frÃ¥n projektroten:

python -m streamlit run app/app.py

âœ… Kvalitet & Robusthet

dbt tester (not_null, m.fl.)

SÃ¤ker SQL-escape i Streamlit

Tydlig matchning mellan geo-data och dbt-marts

Debug-sektion fÃ¶r att visa om kommuner saknar matchning
#
## Streamlit 
python app/geo/process_geo.py
python -m streamlit run app/app.py


ğŸ¯ Sammanfattning

Detta projekt demonstrerar:

End-to-end data engineering

Analytisk modellering med dbt

Geografisk visualisering

Tydlig separation mellan ingestion, transformation och presentation

Allt kÃ¶rbart lokalt, reproducerbart och granskningsbart.
--------------------------